{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:53:43.395417Z","iopub.execute_input":"2025-03-19T10:53:43.395768Z","iopub.status.idle":"2025-03-19T10:53:43.801746Z","shell.execute_reply.started":"2025-03-19T10:53:43.395739Z","shell.execute_reply":"2025-03-19T10:53:43.800687Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Clone the repository\n!git clone https://github.com/hyperopt/hyperopt-sklearn.git\n\n# Change to the directory\n%cd hyperopt-sklearn\n\n# Install the package in editable mode\n!pip install -e .\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:53:43.803016Z","iopub.execute_input":"2025-03-19T10:53:43.803500Z","iopub.status.idle":"2025-03-19T10:54:03.268035Z","shell.execute_reply.started":"2025-03-19T10:53:43.803471Z","shell.execute_reply":"2025-03-19T10:54:03.266726Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'hyperopt-sklearn'...\nremote: Enumerating objects: 3023, done.\u001b[K\nremote: Counting objects: 100% (312/312), done.\u001b[K\nremote: Compressing objects: 100% (114/114), done.\u001b[K\nremote: Total 3023 (delta 212), reused 276 (delta 196), pack-reused 2711 (from 1)\u001b[K\nReceiving objects: 100% (3023/3023), 2.31 MiB | 13.65 MiB/s, done.\nResolving deltas: 100% (1914/1914), done.\n/kaggle/working/hyperopt-sklearn\nObtaining file:///kaggle/working/hyperopt-sklearn\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: hyperopt>=0.2.6 in /usr/local/lib/python3.10/dist-packages (from hpsklearn==1.0.3) (0.2.7)\nRequirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from hpsklearn==1.0.3) (1.26.4)\nCollecting scikit-learn>=1.3.0 (from hpsklearn==1.0.3)\n  Downloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\nRequirement already satisfied: scipy>=1.11.2 in /usr/local/lib/python3.10/dist-packages (from hpsklearn==1.0.3) (1.13.1)\nRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.6->hpsklearn==1.0.3) (1.17.0)\nRequirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.6->hpsklearn==1.0.3) (3.4.2)\nRequirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.6->hpsklearn==1.0.3) (1.0.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.6->hpsklearn==1.0.3) (4.67.1)\nRequirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.6->hpsklearn==1.0.3) (3.1.0)\nRequirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt>=0.2.6->hpsklearn==1.0.3) (0.10.9.7)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.26.0->hpsklearn==1.0.3) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.26.0->hpsklearn==1.0.3) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.26.0->hpsklearn==1.0.3) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.26.0->hpsklearn==1.0.3) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.26.0->hpsklearn==1.0.3) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.26.0->hpsklearn==1.0.3) (2.4.1)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->hpsklearn==1.0.3) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.3.0->hpsklearn==1.0.3) (3.5.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.26.0->hpsklearn==1.0.3) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.26.0->hpsklearn==1.0.3) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.26.0->hpsklearn==1.0.3) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.26.0->hpsklearn==1.0.3) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.26.0->hpsklearn==1.0.3) (2024.2.0)\nDownloading scikit_learn-1.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m:01\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: hpsklearn\n  Building editable for hpsklearn (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for hpsklearn: filename=hpsklearn-1.0.3-0.editable-py3-none-any.whl size=7085 sha256=cf3995e2595758ab1b4fef5ca54fedf3d59c8b0b1f68039c97a0c22fb5cb24f0\n  Stored in directory: /tmp/pip-ephem-wheel-cache-j_ueh4wj/wheels/9a/7b/08/ac8e7a6d0aeee9e6c5b0eddc4f025e5aec4bb780309a122d0c\nSuccessfully built hpsklearn\nInstalling collected packages: scikit-learn, hpsklearn\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed hpsklearn-1.0.3 scikit-learn-1.6.1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install openml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:54:03.270226Z","iopub.execute_input":"2025-03-19T10:54:03.270649Z","iopub.status.idle":"2025-03-19T10:54:11.044435Z","shell.execute_reply.started":"2025-03-19T10:54:03.270607Z","shell.execute_reply":"2025-03-19T10:54:11.042938Z"}},"outputs":[{"name":"stdout","text":"Collecting openml\n  Downloading openml-0.15.1-py3-none-any.whl.metadata (10 kB)\nCollecting liac-arff>=2.4.0 (from openml)\n  Downloading liac-arff-2.5.0.tar.gz (13 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting xmltodict (from openml)\n  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from openml) (2.32.3)\nRequirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.10/dist-packages (from openml) (1.6.1)\nRequirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from openml) (2.9.0.post0)\nRequirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from openml) (2.2.3)\nRequirement already satisfied: scipy>=0.13.3 in /usr/local/lib/python3.10/dist-packages (from openml) (1.13.1)\nRequirement already satisfied: numpy>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from openml) (1.26.4)\nCollecting minio (from openml)\n  Downloading minio-7.2.15-py3-none-any.whl.metadata (6.7 kB)\nRequirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from openml) (19.0.1)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openml) (4.67.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from openml) (24.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.6.2->openml) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.6.2->openml) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.6.2->openml) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.6.2->openml) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.6.2->openml) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.6.2->openml) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->openml) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->openml) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->openml) (1.17.0)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->openml) (1.4.2)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->openml) (3.5.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from minio->openml) (2025.1.31)\nRequirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from minio->openml) (2.3.0)\nRequirement already satisfied: argon2-cffi in /usr/local/lib/python3.10/dist-packages (from minio->openml) (23.1.0)\nRequirement already satisfied: pycryptodome in /usr/local/lib/python3.10/dist-packages (from minio->openml) (3.21.0)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from minio->openml) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->openml) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->openml) (3.10)\nRequirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.10/dist-packages (from argon2-cffi->minio->openml) (21.2.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.6.2->openml) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.6.2->openml) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.6.2->openml) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.6.2->openml) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.6.2->openml) (2024.2.0)\nRequirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from argon2-cffi-bindings->argon2-cffi->minio->openml) (1.17.1)\nRequirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->minio->openml) (2.22)\nDownloading openml-0.15.1-py3-none-any.whl (160 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.4/160.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading minio-7.2.15-py3-none-any.whl (95 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.1/95.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\nBuilding wheels for collected packages: liac-arff\n  Building wheel for liac-arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for liac-arff: filename=liac_arff-2.5.0-py3-none-any.whl size=11716 sha256=a554ee13887cd3e45737ba54e40d97a06f31033ff502ba5a738f00a68d4c5976\n  Stored in directory: /root/.cache/pip/wheels/5d/2a/9c/3895d9617f8f49a0883ba686326d598e78a1c2f54fe3cae86d\nSuccessfully built liac-arff\nInstalling collected packages: xmltodict, liac-arff, minio, openml\nSuccessfully installed liac-arff-2.5.0 minio-7.2.15 openml-0.15.1 xmltodict-0.14.2\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import openml\nimport time\nimport numpy as np\nimport pandas as pd\nfrom hpsklearn import HyperoptEstimator, any_regressor\nfrom hyperopt import tpe\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# ✅ Define datasets to search (OpenML IDs)\ndataset_ids = [ 8, 195,531,204]  # California Housing, Dataset 8, Dataset 531\nresults = []\n\n# ✅ Iterate over datasets\nfor dataset_id in dataset_ids:\n    print(f\"\\n🔍 Loading dataset {dataset_id} from OpenML...\")\n    dataset = openml.datasets.get_dataset(dataset_id)\n    \n    # Get data from OpenML dataset\n    X, y, _, _ = dataset.get_data(target=dataset.default_target_attribute)\n    \n    # ✅ Split data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # ✅ Define Hyperopt-Sklearn Estimator\n    hyperopt_automl = HyperoptEstimator(\n        regressor=any_regressor('reg'),  # Search all regressors\n        algo=tpe.suggest,                # Use TPE (Tree-structured Parzen Estimator)\n        max_evals=50,                     # Increase trials for better performance\n        trial_timeout=300                  # Limit time per model evaluation\n    )\n    \n    # ✅ Train the model\n    print(f\"🚀 Training Hyperopt-Sklearn on dataset {dataset_id}...\")\n    start_time = time.time()\n    hyperopt_automl.fit(X_train.to_numpy(), y_train.to_numpy())  # Convert to NumPy arrays\n    fit_time = time.time() - start_time\n    \n    # ✅ Make predictions\n    y_pred = hyperopt_automl.predict(X_test.to_numpy())\n    \n    # ✅ Evaluate performance\n    mse = mean_squared_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n    \n    # ✅ Store results\n    result = {\n        \"Dataset ID\": dataset_id,\n        \"Hyperopt R² Score\": r2,\n        \"Hyperopt MSE\": mse,\n        \"Hyperopt Time\": fit_time,\n        \"Hyperopt Best Models\": hyperopt_automl.best_model()\n    }\n    results.append(result)\n    \n    print(f\"✅ Completed dataset {dataset_id} -> MSE: {mse:.4f}, R²: {r2:.4f}\")\n\n# ✅ Save results to CSV\nresults_df = pd.DataFrame(results)\nresults_df.to_csv(\"hyperopt_results.csv\", index=False)\n\nprint(\"\\n📁 Results saved to hyperopt_results.csv!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T08:29:30.736655Z","iopub.execute_input":"2025-03-19T08:29:30.737241Z","iopub.status.idle":"2025-03-19T09:30:48.236006Z","shell.execute_reply.started":"2025-03-19T08:29:30.737198Z","shell.execute_reply":"2025-03-19T09:30:48.234189Z"}},"outputs":[{"name":"stdout","text":"\n🔍 Loading dataset 8 from OpenML...\n🚀 Training Hyperopt-Sklearn on dataset 8...\n100%|██████████| 1/1 [00:00<00:00,  4.89trial/s, best loss: 0.8774762464269104]\n100%|██████████| 2/2 [00:00<00:00, 10.28trial/s, best loss: 0.8774762464269104]\n100%|██████████| 3/3 [05:00<00:00, 300.18s/trial, best loss: 0.8774762464269104]\n100%|██████████| 4/4 [00:00<00:00, 10.46trial/s, best loss: 0.7913505582242394]\n100%|██████████| 5/5 [00:00<00:00, 10.33trial/s, best loss: 0.7913505582242394]\n100%|██████████| 6/6 [00:00<00:00,  2.89trial/s, best loss: 0.7913505582242394]\n100%|██████████| 7/7 [00:00<00:00,  9.56trial/s, best loss: 0.7913505582242394]\n100%|██████████| 8/8 [00:00<00:00,  1.56trial/s, best loss: 0.7913505582242394]\n100%|██████████| 9/9 [00:00<00:00,  7.37trial/s, best loss: 0.7913505582242394]\n100%|██████████| 10/10 [00:00<00:00,  8.30trial/s, best loss: 0.7913505582242394]\n100%|██████████| 11/11 [00:00<00:00,  2.32trial/s, best loss: 0.7913505582242394]\n100%|██████████| 12/12 [00:00<00:00,  9.40trial/s, best loss: 0.7913505582242394]\n100%|██████████| 13/13 [00:05<00:00,  5.39s/trial, best loss: 0.7913505582242394]\n100%|██████████| 14/14 [00:00<00:00,  9.18trial/s, best loss: 0.7913505582242394]\n100%|██████████| 15/15 [00:00<00:00,  2.03trial/s, best loss: 0.7913505582242394]\n100%|██████████| 16/16 [00:00<00:00,  2.82trial/s, best loss: 0.7913505582242394]\n100%|██████████| 17/17 [00:00<00:00,  9.68trial/s, best loss: 0.7913505582242394]\n100%|██████████| 18/18 [00:00<00:00,  1.02trial/s, best loss: 0.7913505582242394]\n100%|██████████| 19/19 [00:00<00:00,  7.66trial/s, best loss: 0.7913505582242394]\n100%|██████████| 20/20 [00:00<00:00,  9.57trial/s, best loss: 0.7913505582242394]\n100%|██████████| 21/21 [00:00<00:00,  2.64trial/s, best loss: 0.7697817301284929]\n100%|██████████| 22/22 [05:00<00:00, 300.23s/trial, best loss: 0.7697817301284929]\n100%|██████████| 23/23 [00:00<00:00,  2.71trial/s, best loss: 0.7697817301284929]\n100%|██████████| 24/24 [00:00<00:00,  6.81trial/s, best loss: 0.7697817301284929]\n100%|██████████| 25/25 [00:02<00:00,  2.22s/trial, best loss: 0.7633657496602341]\n100%|██████████| 26/26 [00:02<00:00,  2.23s/trial, best loss: 0.7633657496602341]\n100%|██████████| 27/27 [00:01<00:00,  1.89s/trial, best loss: 0.7437333603943695]\n100%|██████████| 28/28 [00:02<00:00,  2.34s/trial, best loss: 0.7436742349261405]\n100%|██████████| 29/29 [00:00<00:00,  5.29trial/s, best loss: 0.7436742349261405]\n100%|██████████| 30/30 [00:01<00:00,  1.05s/trial, best loss: 0.7436742349261405]\n100%|██████████| 31/31 [00:00<00:00,  5.77trial/s, best loss: 0.7436742349261405]\n100%|██████████| 32/32 [05:00<00:00, 300.48s/trial, best loss: 0.7436742349261405]\n100%|██████████| 33/33 [00:02<00:00,  2.31s/trial, best loss: 0.7436742349261405]\n100%|██████████| 34/34 [00:00<00:00,  6.91trial/s, best loss: 0.7436742349261405]\n100%|██████████| 35/35 [00:00<00:00,  4.06trial/s, best loss: 0.7436742349261405]\n100%|██████████| 36/36 [00:01<00:00,  1.03s/trial, best loss: 0.7436742349261405]\n100%|██████████| 37/37 [00:00<00:00,  2.48trial/s, best loss: 0.7436742349261405]\n100%|██████████| 38/38 [00:00<00:00,  4.01trial/s, best loss: 0.7436742349261405]\n100%|██████████| 39/39 [00:00<00:00,  2.52trial/s, best loss: 0.7436742349261405]\n100%|██████████| 40/40 [05:00<00:00, 300.21s/trial, best loss: 0.7436742349261405]\n100%|██████████| 41/41 [00:01<00:00,  1.28s/trial, best loss: 0.7436742349261405]\n100%|██████████| 42/42 [00:00<00:00,  6.37trial/s, best loss: 0.7436742349261405]\n100%|██████████| 43/43 [00:00<00:00,  4.10trial/s, best loss: 0.7436742349261405]\n100%|██████████| 44/44 [00:04<00:00,  4.37s/trial, best loss: 0.7436742349261405]\n100%|██████████| 45/45 [00:00<00:00,  2.11trial/s, best loss: 0.7436742349261405]\n100%|██████████| 46/46 [00:00<00:00,  2.49trial/s, best loss: 0.7436742349261405]\n100%|██████████| 47/47 [00:01<00:00,  1.56s/trial, best loss: 0.6976104874059909]\n100%|██████████| 48/48 [05:00<00:00, 300.21s/trial, best loss: 0.6976104874059909]\n100%|██████████| 49/49 [00:00<00:00,  2.12trial/s, best loss: 0.6976104874059909]\n 98%|█████████▊| 49/50 [00:00<?, ?trial/s, best loss=?]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_regression.py:1275: RuntimeWarning: overflow encountered in square\n  numerator = xp.sum(weight * (y_true - y_pred) ** 2, axis=0)\n\n","output_type":"stream"},{"name":"stdout","text":"100%|██████████| 50/50 [00:01<00:00,  1.85s/trial, best loss: 0.6976104874059909]\n✅ Completed dataset 8 -> MSE: 9.4551, R²: 0.1118\n\n🔍 Loading dataset 195 from OpenML...\n🚀 Training Hyperopt-Sklearn on dataset 195...\n100%|██████████| 1/1 [00:00<00:00,  9.95trial/s, best loss: 0.42875907030665994]\n100%|██████████| 2/2 [00:00<00:00,  9.67trial/s, best loss: 0.2720792758470941]\n100%|██████████| 3/3 [00:00<00:00,  7.08trial/s, best loss: 0.2378487424083594]\n100%|██████████| 4/4 [00:00<00:00,  9.38trial/s, best loss: 0.2378487424083594]\n100%|██████████| 5/5 [00:00<00:00,  2.95trial/s, best loss: 0.2378487424083594]\n100%|██████████| 6/6 [00:01<00:00,  1.94s/trial, best loss: 0.22269897983806042]\n100%|██████████| 7/7 [00:00<00:00,  2.92trial/s, best loss: 0.22269897983806042]\n100%|██████████| 8/8 [00:00<00:00,  9.98trial/s, best loss: 0.22269897983806042]\n100%|██████████| 9/9 [00:00<00:00,  5.22trial/s, best loss: 0.22269897983806042]\n100%|██████████| 10/10 [00:00<00:00,  2.43trial/s, best loss: 0.19501145460206548]\n100%|██████████| 11/11 [05:00<00:00, 300.13s/trial, best loss: 0.19501145460206548]\n100%|██████████| 12/12 [00:00<00:00,  2.60trial/s, best loss: 0.19501145460206548]\n100%|██████████| 13/13 [05:00<00:00, 300.18s/trial, best loss: 0.19501145460206548]\n100%|██████████| 14/14 [00:00<00:00, 10.10trial/s, best loss: 0.1783250173225669]\n100%|██████████| 15/15 [00:00<00:00,  2.63trial/s, best loss: 0.1783250173225669]\n100%|██████████| 16/16 [00:00<00:00,  1.89trial/s, best loss: 0.1783250173225669]\n100%|██████████| 17/17 [00:01<00:00,  1.50s/trial, best loss: 0.1783250173225669]\n100%|██████████| 18/18 [00:00<00:00,  1.57trial/s, best loss: 0.1783250173225669]\n100%|██████████| 19/19 [00:00<00:00,  9.22trial/s, best loss: 0.1783250173225669]\n100%|██████████| 20/20 [00:00<00:00, 10.00trial/s, best loss: 0.1783250173225669]\n100%|██████████| 21/21 [00:00<00:00,  6.98trial/s, best loss: 0.1783250173225669]\n100%|██████████| 22/22 [00:00<00:00,  2.42trial/s, best loss: 0.1783250173225669]\n100%|██████████| 23/23 [00:00<00:00,  6.86trial/s, best loss: 0.1783250173225669]\n100%|██████████| 24/24 [05:00<00:00, 300.23s/trial, best loss: 0.1783250173225669]\n100%|██████████| 25/25 [00:00<00:00,  4.99trial/s, best loss: 0.1783250173225669]\n100%|██████████| 26/26 [00:00<00:00,  6.53trial/s, best loss: 0.1783250173225669]\n100%|██████████| 27/27 [00:00<00:00,  6.85trial/s, best loss: 0.1783250173225669]\n100%|██████████| 28/28 [00:03<00:00,  3.72s/trial, best loss: 0.1783250173225669]\n100%|██████████| 29/29 [00:00<00:00,  2.55trial/s, best loss: 0.1783250173225669]\n100%|██████████| 30/30 [00:00<00:00,  6.70trial/s, best loss: 0.1783250173225669]\n100%|██████████| 31/31 [05:00<00:00, 300.24s/trial, best loss: 0.1783250173225669]\n100%|██████████| 32/32 [00:00<00:00,  6.38trial/s, best loss: 0.1783250173225669]\n100%|██████████| 33/33 [00:00<00:00,  5.48trial/s, best loss: 0.1783250173225669]\n100%|██████████| 34/34 [00:00<00:00,  6.02trial/s, best loss: 0.1783250173225669]\n100%|██████████| 35/35 [00:00<00:00,  2.27trial/s, best loss: 0.1783250173225669]\n100%|██████████| 36/36 [00:00<00:00,  3.03trial/s, best loss: 0.1783250173225669]\n100%|██████████| 37/37 [00:00<00:00,  6.46trial/s, best loss: 0.1783250173225669]\n100%|██████████| 38/38 [00:00<00:00,  2.58trial/s, best loss: 0.1783250173225669]\n100%|██████████| 39/39 [05:00<00:00, 300.22s/trial, best loss: 0.1783250173225669]\n100%|██████████| 40/40 [00:00<00:00,  2.43trial/s, best loss: 0.1783250173225669]\n100%|██████████| 41/41 [00:00<00:00,  4.60trial/s, best loss: 0.1783250173225669]\n100%|██████████| 42/42 [05:00<00:00, 300.20s/trial, best loss: 0.1783250173225669]\n100%|██████████| 43/43 [00:00<00:00,  5.32trial/s, best loss: 0.1783250173225669]\n100%|██████████| 44/44 [00:00<00:00,  6.10trial/s, best loss: 0.1783250173225669]\n 98%|█████████▊| 44/45 [00:00<?, ?trial/s, best loss=?]","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_stochastic_gradient.py:1608: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n  warnings.warn(\n\n","output_type":"stream"},{"name":"stdout","text":"100%|██████████| 45/45 [00:00<00:00,  2.30trial/s, best loss: 0.1783250173225669]\n100%|██████████| 46/46 [00:00<00:00,  6.36trial/s, best loss: 0.1783250173225669]\n100%|██████████| 47/47 [00:00<00:00,  2.58trial/s, best loss: 0.1783250173225669]\n100%|██████████| 48/48 [00:00<00:00,  3.78trial/s, best loss: 0.1783250173225669]\n100%|██████████| 49/49 [00:01<00:00,  1.04s/trial, best loss: 0.1783250173225669]\n100%|██████████| 50/50 [00:02<00:00,  2.66s/trial, best loss: 0.1783250173225669]\n✅ Completed dataset 195 -> MSE: 6224542.1875, R²: 0.6502\n\n🔍 Loading dataset 531 from OpenML...\n🚀 Training Hyperopt-Sklearn on dataset 531...\n100%|██████████| 1/1 [05:00<00:00, 300.14s/trial, best loss=?]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAllTrialsFailed\u001b[0m                           Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-fc37f902afb8>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"🚀 Training Hyperopt-Sklearn on dataset {dataset_id}...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0mhyperopt_automl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Convert to NumPy arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mfit_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/hyperopt-sklearn/hyperopt-sklearn/hpsklearn/estimator/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, EX_list, valid_size, n_folds, kfolds_group, cv_shuffle, warm_start, random_state)\u001b[0m\n\u001b[1;32m    478\u001b[0m                 increment = min(self.fit_increment,\n\u001b[1;32m    479\u001b[0m                                 adjusted_max_evals - len(self.trials.trials))\n\u001b[0;32m--> 480\u001b[0;31m                 \u001b[0mfit_iter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mincrement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_increment_dump_filename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/working/hyperopt-sklearn/hyperopt-sklearn/hpsklearn/estimator/estimator.py\u001b[0m in \u001b[0;36mfit_iter\u001b[0;34m(self, X, y, EX_list, valid_size, n_folds, kfolds_group, cv_shuffle, warm_start, random_state)\u001b[0m\n\u001b[1;32m    345\u001b[0m             \u001b[0;31m# Workaround for rstate issue #35\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"rstate\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m                 hyperopt.fmin(_fn_with_timeout,\n\u001b[0m\u001b[1;32m    348\u001b[0m                               \u001b[0mspace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                               \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mallow_trials_fmin\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fmin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 540\u001b[0;31m         return trials.fmin(\n\u001b[0m\u001b[1;32m    541\u001b[0m             \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfmin\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         return fmin(\n\u001b[0m\u001b[1;32m    672\u001b[0m             \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[1;32m    595\u001b[0m         \u001b[0;31m# Only if there are some successful trail runs, return the best point in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m         \u001b[0;31m# the evaluation space\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mspace_eval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36margmin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0margmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m         \u001b[0mbest_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m         \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_trial\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"misc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vals\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;31m# unpack the one-element lists to values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mbest_trial\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    609\u001b[0m         ]\n\u001b[1;32m    610\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAllTrialsFailed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m         \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"result\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAllTrialsFailed\u001b[0m: "],"ename":"AllTrialsFailed","evalue":"","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"import openml\nimport time\nimport numpy as np\nimport pandas as pd\nfrom hpsklearn import HyperoptEstimator, any_regressor\nfrom hyperopt import tpe\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# ✅ Define datasets to search (OpenML IDs)\ndataset_ids = [ 204]  # California Housing, Dataset 8, Dataset 531\nresults = []\n\n# ✅ Iterate over datasets\nfor dataset_id in dataset_ids:\n    print(f\"\\n🔍 Loading dataset {dataset_id} from OpenML...\")\n    dataset = openml.datasets.get_dataset(dataset_id)\n    \n    # Get data from OpenML dataset\n    X, y, _, _ = dataset.get_data(target=dataset.default_target_attribute)\n    mask = X.isna().any(axis=1) | y.isna()\n\n    # Drop those rows from both X and y\n    X = X[~mask]\n    y = y[~mask]\n    \n    # ✅ Split data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    \n    # ✅ Define Hyperopt-Sklearn Estimator\n    hyperopt_automl = HyperoptEstimator(\n        regressor=any_regressor('reg'),  # Search all regressors\n        algo=tpe.suggest,                # Use TPE (Tree-structured Parzen Estimator)\n        max_evals=50,                     # Increase trials for better performance\n        trial_timeout=300                  # Limit time per model evaluation\n    )\n    \n    # ✅ Train the model\n    print(f\"🚀 Training Hyperopt-Sklearn on dataset {dataset_id}...\")\n    start_time = time.time()\n    hyperopt_automl.fit(X_train.to_numpy(), y_train.to_numpy())  # Convert to NumPy arrays\n    fit_time = time.time() - start_time\n    \n    # ✅ Make predictions\n    y_pred = hyperopt_automl.predict(X_test.to_numpy())\n    \n    # ✅ Evaluate performance\n    mse = mean_squared_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n    \n    # ✅ Store results\n    result = {\n        \"Dataset ID\": dataset_id,\n        \"Hyperopt R² Score\": r2,\n        \"Hyperopt MSE\": mse,\n        \"Hyperopt Time\": fit_time,\n        \"Hyperopt Best Models\": hyperopt_automl.best_model()\n    }\n    results.append(result)\n    \n    print(f\"✅ Completed dataset {dataset_id} -> MSE: {mse:.4f}, R²: {r2:.4f}\")\n\n# ✅ Save results to CSV\nresults_df = pd.DataFrame(results)\nresults_df.to_csv(\"hyperopt_results.csv\", index=False)\n\nprint(\"\\n📁 Results saved to hyperopt_results.csv!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T10:59:18.549740Z","iopub.execute_input":"2025-03-19T10:59:18.550156Z","iopub.status.idle":"2025-03-19T10:59:49.288646Z","shell.execute_reply.started":"2025-03-19T10:59:18.550127Z","shell.execute_reply":"2025-03-19T10:59:49.287488Z"}},"outputs":[{"name":"stdout","text":"\n🔍 Loading dataset 204 from OpenML...\n🚀 Training Hyperopt-Sklearn on dataset 204...\n100%|██████████| 1/1 [00:00<00:00,  4.48trial/s, best loss: 0.99110866924139]\n100%|██████████| 2/2 [00:02<00:00,  2.20s/trial, best loss: 0.99110866924139]\n100%|██████████| 3/3 [00:00<00:00,  2.80trial/s, best loss: 0.99110866924139]\n100%|██████████| 4/4 [00:00<00:00,  8.83trial/s, best loss: 0.99110866924139]\n100%|██████████| 5/5 [00:02<00:00,  2.58s/trial, best loss: 0.99110866924139]\n100%|██████████| 6/6 [00:00<00:00,  1.71trial/s, best loss: 0.99110866924139]\n100%|██████████| 7/7 [00:00<00:00, 10.55trial/s, best loss: 0.99110866924139]\n100%|██████████| 8/8 [00:00<00:00,  1.52trial/s, best loss: 0.9788300089158094]\n100%|██████████| 9/9 [00:00<00:00, 10.19trial/s, best loss: 0.9788300089158094]\n100%|██████████| 10/10 [00:00<00:00,  5.36trial/s, best loss: 0.9788300089158094]\n100%|██████████| 11/11 [00:00<00:00,  1.28trial/s, best loss: 0.9788300089158094]\n100%|██████████| 12/12 [00:00<00:00,  9.39trial/s, best loss: 0.9788300089158094]\n100%|██████████| 13/13 [00:00<00:00,  1.01trial/s, best loss: 0.9788300089158094]\n100%|██████████| 14/14 [00:00<00:00,  8.61trial/s, best loss: 0.9788300089158094]\n100%|██████████| 15/15 [00:00<00:00,  7.78trial/s, best loss: 0.9767782814046753]\n100%|██████████| 16/16 [00:00<00:00,  8.18trial/s, best loss: 0.9767782814046753]\n100%|██████████| 17/17 [00:00<00:00,  1.24trial/s, best loss: 0.9494719228164459]\n100%|██████████| 18/18 [00:00<00:00,  3.18trial/s, best loss: 0.9494719228164459]\n100%|██████████| 19/19 [00:01<00:00,  1.76s/trial, best loss: 0.9494719228164459]\n100%|██████████| 20/20 [00:00<00:00,  2.69trial/s, best loss: 0.9494719228164459]\n100%|██████████| 21/21 [00:00<00:00,  1.35trial/s, best loss: 0.9494719228164459]\n100%|██████████| 22/22 [00:00<00:00,  1.04trial/s, best loss: 0.9352643434381871]\n100%|██████████| 23/23 [00:00<00:00,  1.53trial/s, best loss: 0.9352643434381871]\n100%|██████████| 24/24 [00:00<00:00,  4.81trial/s, best loss: 0.9352643434381871]\n100%|██████████| 25/25 [00:00<00:00,  7.17trial/s, best loss: 0.8992372324175311]\n100%|██████████| 26/26 [00:00<00:00,  7.14trial/s, best loss: 0.8992372324175311]\n100%|██████████| 27/27 [00:00<00:00,  2.77trial/s, best loss: 0.8992372324175311]\n100%|██████████| 28/28 [00:00<00:00,  7.28trial/s, best loss: 0.8992372324175311]\n100%|██████████| 29/29 [00:00<00:00,  2.89trial/s, best loss: 0.8992372324175311]\n100%|██████████| 30/30 [00:00<00:00,  7.02trial/s, best loss: 0.8992372324175311]\n100%|██████████| 31/31 [00:00<00:00,  7.16trial/s, best loss: 0.8992372324175311]\n100%|██████████| 32/32 [00:00<00:00,  7.24trial/s, best loss: 0.8992372324175311]\n100%|██████████| 33/33 [00:00<00:00,  6.78trial/s, best loss: 0.8992372324175311]\n100%|██████████| 34/34 [00:00<00:00,  2.69trial/s, best loss: 0.8992372324175311]\n100%|██████████| 35/35 [00:00<00:00,  6.36trial/s, best loss: 0.8992372324175311]\n100%|██████████| 36/36 [00:00<00:00,  2.67trial/s, best loss: 0.8992372324175311]\n100%|██████████| 37/37 [00:00<00:00,  7.17trial/s, best loss: 0.8992372324175311]\n100%|██████████| 38/38 [00:03<00:00,  3.50s/trial, best loss: 0.8992372324175311]\n100%|██████████| 39/39 [00:00<00:00,  1.28trial/s, best loss: 0.8992372324175311]\n100%|██████████| 40/40 [00:00<00:00,  6.69trial/s, best loss: 0.8992372324175311]\n100%|██████████| 41/41 [00:00<00:00,  6.03trial/s, best loss: 0.8992372324175311]\n100%|██████████| 42/42 [00:00<00:00,  7.08trial/s, best loss: 0.8992372324175311]\n100%|██████████| 43/43 [00:00<00:00,  2.49trial/s, best loss: 0.8992372324175311]\n100%|██████████| 44/44 [00:00<00:00,  6.50trial/s, best loss: 0.8992372324175311]\n100%|██████████| 45/45 [00:00<00:00,  2.63trial/s, best loss: 0.8992372324175311]\n100%|██████████| 46/46 [00:00<00:00,  3.25trial/s, best loss: 0.8992372324175311]\n100%|██████████| 47/47 [00:00<00:00,  1.84trial/s, best loss: 0.8992372324175311]\n100%|██████████| 48/48 [00:00<00:00,  6.74trial/s, best loss: 0.8992372324175311]\n100%|██████████| 49/49 [00:00<00:00,  6.87trial/s, best loss: 0.8992372324175311]\n100%|██████████| 50/50 [00:00<00:00,  7.14trial/s, best loss: 0.8992372324175311]\n✅ Completed dataset 204 -> MSE: 2674.6323, R²: -0.0648\n\n📁 Results saved to hyperopt_results.csv!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T11:06:52.007285Z","iopub.execute_input":"2025-03-19T11:06:52.007793Z","iopub.status.idle":"2025-03-19T11:06:52.017198Z","shell.execute_reply.started":"2025-03-19T11:06:52.007760Z","shell.execute_reply":"2025-03-19T11:06:52.016276Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"{'Dataset ID': 204,\n 'Hyperopt R² Score': -0.06479912778759478,\n 'Hyperopt MSE': 2674.632252675257,\n 'Hyperopt Time': 30.684821605682373,\n 'Hyperopt Best Models': {'learner': ExtraTreeRegressor(criterion='friedman_mse', max_depth=3, max_features='log2',\n                     max_leaf_nodes=15, random_state=3, splitter='best'),\n  'preprocs': (Normalizer(),),\n  'ex_preprocs': ()}}"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"import openml","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T15:44:35.548372Z","iopub.execute_input":"2025-03-19T15:44:35.548651Z","iopub.status.idle":"2025-03-19T15:44:35.625876Z","shell.execute_reply.started":"2025-03-19T15:44:35.548617Z","shell.execute_reply":"2025-03-19T15:44:35.624650Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-9d0b98397986>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mopenml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openml'"],"ename":"ModuleNotFoundError","evalue":"No module named 'openml'","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}